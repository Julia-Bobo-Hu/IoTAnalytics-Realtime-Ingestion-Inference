{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img src=\"https://s3.amazonaws.com/iotanalytics-templates/Logo.png\" style=\"float:left;\">\n",
    "    <h1 style=\"color:#1A5276;padding-left:115px;padding-bottom:0px;font-size:28px;\">AWS IoT Analytics | Building Energy Consumption Prediction</h1>\n",
    "</p>\n",
    "<p style=\"color:#1A5276;padding-left:90px;padding-top:0px;position:relative;font-style:italic;font-size:18px\">\n",
    "Applying model trained from the training job to automatically predict the building energy consumption and Output Dataset to IoT Analytics Dataset.   \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: Import Required Notebook Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook uses holidays package\n",
    "\n",
    "try:\n",
    "    import holidays\n",
    "    import lightgbm as lgb\n",
    "except:\n",
    "    !pip install holidays\n",
    "    import holidays\n",
    "    !pip install lightgbm\n",
    "    import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#20B3CD;font-size:20px;float:left\">Step 1  |  Load Meter-reading and weather Data from IoTAnalytics</h1> <div style=\"float:right;height:7px;background-color:#20B3CD;margin-top:30px;width:70%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before actually loading the data we need to set up an IoT Analytics client for accessing datasets.\n",
    "# create IoT Analytics client\n",
    "client = boto3.client('iotanalytics')\n",
    "\n",
    "dataset = \"jh_iot_analytics_data_set\"\n",
    "dataset_url = client.get_dataset_content(datasetName = dataset)['entries'][0]['dataURI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can get the data location (URL) for the given dataset and start working with the data (In order to need to perform get_dataset_content, you need to grant iot analytics corresponding IAM permission):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start working with the data\n",
    "test_df = pd.read_csv(dataset_url,parse_dates=True)\n",
    "if test_df.empty:\n",
    "    raise Exception('No data found')\n",
    "    \n",
    "# start working with the data\n",
    "drop_col = ['meter_reading','__dt']\n",
    "test_df.drop(drop_col, axis=1, inplace=True) # removes unnecessary columns\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'] / 1000., unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset = \"iot_analytics_data_set\"\n",
    "weather_dataset_url = client.get_dataset_content(datasetName = weather_dataset)['entries'][0]['dataURI']\n",
    "\n",
    "# start working with the data\n",
    "weather_test_df = pd.read_csv(weather_dataset_url,parse_dates=True)\n",
    "if weather_test_df.empty:\n",
    "    raise Exception('No data found')\n",
    "    \n",
    "# start working with the data\n",
    "drop_col = ['__dt']\n",
    "weather_test_df.drop(drop_col, axis=1, inplace=True) # removes unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#20B3CD;font-size:20px;float:left\">Step 2  |  Feature Engineering</h1> <div style=\"float:right;height:7px;background-color:#20B3CD;margin-top:30px;width:70%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Weathertransformer\n",
    "Added missing time-series data by finding start_date-end_date\n",
    "Then fill in missed data invweather data, temperature, cloud coverage, due_temperature, sea_level, wind_direction, wind_speed, precip_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weathertranformer import WeatherTranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test_df = WeatherTranformer(True).fit_transform(weather_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Smoothing Filter\n",
    "Smooth air and dew temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test_df.dropna(subset=['air_temperature'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SGFilter import SGFilterTranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test_df = SGFilterTranformer(True).fit_transform(weather_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Rolling Window\n",
    "Calculate min max std within a time window of 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Rollwindow import RollwinTranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test_df = RollwinTranformer(True,24).fit_transform(weather_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Get Building related metadata from S3 bucket and Merge all datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"check-ride-data-explore\"\n",
    "file_name = \"input/ashrae-energy-prediction/building_metadata.csv\"\n",
    "\n",
    "s3 = boto3.client('s3') \n",
    "# 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "# get object and file (key) from bucket\n",
    "\n",
    "building_df = pd.read_csv(obj['Body']) # 'Body' is a key word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test_df['timestamp'] = pd.to_datetime(weather_test_df['timestamp'])\n",
    "test_df = test_df.merge(weather_test_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Numerical Features\n",
    "Feature transform for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NumericalEng import NumericalTransformer\n",
    "test_df = NumericalTransformer(True, True, True, True).fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_df = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Holidays Features\n",
    "Add one feature to state if that day is public holiday or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HolidayFea import HolidayTranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = HolidayTranformer(True).fit_transform(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) One hot Encoding for Primary use\n",
    "One hot encoding for categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LabelEncode import CategoricalTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dropna(subset=['building_id'],inplace=True)\n",
    "test_df = CategoricalTransformer().fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#20B3CD;font-size:20px;float:left\">Step 3  |  Generate Prediction</h1> <div style=\"float:right;height:7px;background-color:#20B3CD;margin-top:30px;width:70%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Setup test_features and load model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "gbm_pickle = joblib.load('lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gbm_pickle.predict(test_df, num_iteration=gbm_pickle.best_iteration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'pred':results})\n",
    "pred_df.loc[pred_df.pred < 0, 'pred'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_df = pd.concat([test_time_df,pred_df], axis=1, sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#20B3CD;font-size:20px;float:left\">Step 4  |  Create Dataset to IoT Dataset</h1> <div style=\"float:right;height:7px;background-color:#20B3CD;margin-top:30px;width:70%\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6BDBB2CEE92130CA',\n",
       "  'HostId': 't08zjP6AoG0f1sk7DfHM0sYVT57qg7LSi/WrG5lYtZ27XLYAcovz+KgQFfxvE+xalJ0YHUOk8a0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 't08zjP6AoG0f1sk7DfHM0sYVT57qg7LSi/WrG5lYtZ27XLYAcovz+KgQFfxvE+xalJ0YHUOk8a0=',\n",
       "   'x-amz-request-id': '6BDBB2CEE92130CA',\n",
       "   'date': 'Mon, 31 Aug 2020 05:02:01 GMT',\n",
       "   'etag': '\"cf04317f65d910f98aa864138ba4aa9e\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"cf04317f65d910f98aa864138ba4aa9e\"'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "bucket='check-ride-data-explore'\n",
    "csv_key='prediction_result.csv'\n",
    "prefix = 'energy_prediction' + datetime.now().strftime('%Y-%m-%d') + \"/\"\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "csv_buffer = StringIO()\n",
    "test_final_df.to_csv(csv_buffer)\n",
    "s3.Object(bucket, prefix + csv_key).put(Body = csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:60px;\"><div style=\"height:7px;background-color:#20B3CD;width:100%;margin-top:20px;position:relative;\"><img src=\"https://s3.amazonaws.com/iotanalytics-templates/Logo.png\" style=\"height:50px;width:50px;margin-top:-20px;position:absolute;margin-left:42%;\"></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Containerized conda_python3",
   "language": "python",
   "name": "containerized_conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
